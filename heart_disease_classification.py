# -*- coding: utf-8 -*-
"""HEART_DISEASE_CLASSIFICATION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zzuCFNDKQ1hg2sohP9DhsMdctU4nA4LF
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier 
from sklearn.metrics import classification_report,mean_squared_error,roc_auc_score,confusion_matrix,f1_score,accuracy_score,ConfusionMatrixDisplay

from google.colab import files
uploaded=files.upload()

df = pd.read_csv("heart.csv")
df

df.info()

df.describe()

#Check if their is missing values
df.isna().sum()

df.target.value_counts()

#Some plots of the target column
ax=sns.countplot(x=df['target'], data=df);
plt.title('Presence of heart disease in the patient')
plt.show()
#plt.title('Target column Distribution')
plt.pie(df['target'].value_counts(),labels = ['No Disease', 'Disease'],autopct='%.1f%%',explode=(0,0.1),startangle=90,shadow=True)
plt.show()

#Check relations between columns using pairplot
plt.figure(figsize=(30,30))
sns.pairplot(df)

#Check outliers using boxplot
plt.figure(figsize=(1,1))
c='red'
for i in df.columns:
    sns.boxplot(y=df[i],color=c)
    plt.title(i)
    plt.show()

#Dealing with the outliers values by tukey fences
outliers_features = ['trestbps','chol']
for i in outliers_features:
    #Finding upper and lower limt for features in train set
    Inter_Quartile_Range = df[i].quantile(0.75) - df[i].quantile(0.25)

    lower_boundary = df[i].quantile(0.25) - (Inter_Quartile_Range * 1.5)
    upper_boundary = df[i].quantile(0.75) + (Inter_Quartile_Range * 1.5)
    
    df[i]= np.where(df[i] > upper_boundary, upper_boundary,np.where(df[i] < lower_boundary, lower_boundary,df[i]))

#Boxplot after dealing with outliers
plt.figure(figsize=(5,5))
c='red'
for i in outliers_features:
    sns.boxplot(y=df[i],color=c)
    plt.title(i)
    plt.show()

#Plotting histograms to check distributions of the attributes
plt.figure(figsize=(3,3))
for i in df.columns:
    plt.hist(df[i], color='pink', edgecolor='black', linewidth=2)
    plt.title(i)
    plt.show()

#Check correlation between the attributes with each other
plt.figure(figsize=(15,15))
sns.heatmap(df.corr(),annot=True,cmap="Blues")

#Drop the attributes that have values near to zero
df2 = df.drop(['fbs'],axis=1)
df2

#Divide the data into features (X) and target (y)
X = df2.drop(['target'],axis=1)
y = df2.target

#Split data into train and validation
from sklearn.model_selection import train_test_split  

x_train, x_val, y_train, y_val = train_test_split(X, y, test_size= 0.2, random_state=42)

#Normalize the data using Minmax scaler
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_val = scaler.transform(x_val)

#Modeling
def data_training(x_train, x_val, y_train, y_val):

    models = []
    models.append(('Decision Tree',DecisionTreeClassifier(max_depth=10)))
    models.append(('Logistic regression',LogisticRegression()))
    models.append(('SVM',SVC()))
    df_result = pd.DataFrame(columns=["Model"])
    index = 0
    for name,model in models:
        model.fit(x_train,y_train)
        y_pred = model.predict(x_train)
        y_pred2 = model.predict(x_val)
        train_f1 = f1_score(y_pred, y_train)
        test_f1 = f1_score(y_pred2, y_val)
        df_result.at[index,['Model',"F1 score for training set (%)","F1 score for testing set (%)"]] = [name,round(train_f1*100,1),round(test_f1*100,1)]
        index += 1
    return df_result.sort_values("F1 score for testing set (%)",ascending=False)

dt = data_training(x_train, x_val, y_train, y_val)
dt

#Evaluation
model=DecisionTreeClassifier(max_depth=10)
model.fit(x_train,y_train)
y_pred = model.predict(x_val)

cfm = confusion_matrix(y_val, y_pred=y_pred)
print(cfm)
ConfusionMatrixDisplay.from_estimator(model,x_val, y_val)  
plt.show()
tn, fp, fn, tp = cfm.ravel()
print("True Negatives: ",tn)
print("False Positives: ",fp)
print("False Negatives: ",fn)
print("True Positives: ",tp)

print(classification_report(y_val,y_pred,digits=2))

a=accuracy_score(y_val, model.predict(x_val))
print("Accuracy of the HEART DISEASE CLASSIFICATION MODEL:")
print(a*100)